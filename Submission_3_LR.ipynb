{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw JSONL:\n",
      "- TRAIN: C:\\Users\\39392\\Desktop\\università\\data science\\fundamentals_of_data_science\\progetto\\train.jsonl\n",
      "- TEST : C:\\Users\\39392\\Desktop\\università\\data science\\fundamentals_of_data_science\\progetto\\test.jsonl\n",
      "Loaded: train=10000 battles, test=5000 battles\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cb5960ea834ff7a194082871b31fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting 5 simple features:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f220189a9704d1f9f99f6c8740f5392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting 5 simple features:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FINAL] train_df: (10000, 7)\n",
      "[FINAL] test_df : (5000, 6)\n"
     ]
    }
   ],
   "source": [
    "# This code has been produced by the group \"team_pk\" group, composed of Leonardo Suriano, Riccardo Pugliese and Mariana Dos Campos.\n",
    "# In the code we have provided some comments, that aims to help the reader moving around the code and get what the code is doing.\n",
    "# The whole explanation has been given for each step of the code, from the building features code to the final predictive model. \n",
    "# Moreover, we decided to import libraries not all at once, but to import in every cell the libraries that the cell is using. This choice \n",
    "# has been made in order to make clear which library has been used in that specific cell.\n",
    "# In case the comments we added are not enough to satisfy your curiosity, and in case you may need further clarification about function\n",
    "# taken from libraries, please refer to the documentation of the respective libraries.\n",
    "# In case you need further clarification about function we created from scratch in our code or about how the libraries functions has\n",
    "# been used, please feel free to contact us. We will be more than happy to answer all your doubt!!!\n",
    "\n",
    "\n",
    "# AI assistance disclaimer\n",
    "# Parts of this code (in particular some comments, the iterative feature search, and minor implementation details) may have been drafted or refined \n",
    "# with the help of AI-based tools. The use of AI was strictly limited to these aspects. All core ideas, modeling choices, and logical structures \n",
    "# implemented in the code and in the models were entirely conceived and designed by the members of the group, without external intellectual \n",
    "# contribution, relying solely on online documentation, our own knowledge and the insights provided by the course lectures.\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# Train and test dataset input paths\n",
    "# =========================\n",
    "\n",
    "DATA_DIR   = Path(r\"C:\\Users\\39392\\Desktop\\università\\data science\\fundamentals_of_data_science\\progetto\")\n",
    "TRAIN_FILE = DATA_DIR / \"train.jsonl\"   \n",
    "TEST_FILE  = DATA_DIR / \"test.jsonl\" \n",
    "\n",
    "def load_jsonl(path: Path) -> list[dict]:\n",
    "    with Path(path).open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# =========================\n",
    "# Status severity mapping (since pokemon status is a string, here we are mapping it to numeric).\n",
    "# The bigger the number, the \"worse\" the status.\n",
    "#\n",
    "# - 0 = no status or faint (we treat faint separately in other logic)\n",
    "# - 1 = mild status (paralysis, burn, poison)\n",
    "# - 2 = severe (toxic, freeze)\n",
    "# - 3 = sleep (very strong in Gen 1)\n",
    "#\n",
    "# The following table may have been taken from internet, formatted as below and \n",
    "# copy pasted here.\n",
    "# =========================\n",
    "MAP_STATUS = {\n",
    "    \"nostatus\": 0,\n",
    "    \"par\": 1,\n",
    "    \"brn\": 1,\n",
    "    \"psn\": 1,\n",
    "    \"tox\": 2,\n",
    "    \"frz\": 2,\n",
    "    \"slp\": 3,\n",
    "    \"fnt\": 0,\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Generator for the 5 features we are using in this model\n",
    "# =========================\n",
    "def create_features1(data: list[dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Here we are generating the five features we will use:\n",
    "      - hp_edge_final: HP edge (mean HP of player 2 - mean HP of player 1)\n",
    "      - used_count_diff: difference in the number of (distinct) pokemon used by the two players\n",
    "      - status_severity_gap_final: final status severity difference (player 2 - player 1)\n",
    "      - revealed_count_diff: difference in the number of (distinct) pokemon revealed by the two players\n",
    "      - p1_status_mean_final: mean final status severity for player 1\n",
    "    The train dataset contains clearly also battle_id and player_won.\n",
    "    The test dataset do not contain player_won.\n",
    "    \"\"\"\n",
    "\n",
    "    # now we are starting extracting our features\n",
    "    rows = []\n",
    "    for battle in tqdm(data, desc=\"Extracting 5 simple features\"):\n",
    "        feats: dict = {}\n",
    "\n",
    "        timeline = battle.get(\"battle_timeline\") or []\n",
    "        n_turns = len(timeline)\n",
    "\n",
    "        # Battle identifier + target label (if present)\n",
    "        feats[\"battle_id\"] = battle.get(\"battle_id\")\n",
    "        if \"player_won\" in battle:\n",
    "            feats[\"player_won\"] = int(battle[\"player_won\"])\n",
    "\n",
    "        # -----------------------------\n",
    "        # Basic sequences extracted from the timeline (active Pokémon names for each player)\n",
    "        # -----------------------------\n",
    "        p1_active = [\n",
    "            str(t[\"p1_pokemon_state\"][\"name\"]).lower()\n",
    "            for t in timeline\n",
    "        ]\n",
    "        p2_active = [\n",
    "            str(t[\"p2_pokemon_state\"][\"name\"]).lower()\n",
    "            for t in timeline\n",
    "        ]\n",
    "\n",
    "        # Raw status sequences for completeness (we will only use the final status for each species)\n",
    "        p1_status_raw = [\n",
    "            t[\"p1_pokemon_state\"].get(\"status\", \"nostatus\")\n",
    "            for t in timeline\n",
    "        ]\n",
    "        p2_status_raw = [\n",
    "            t[\"p2_pokemon_state\"].get(\"status\", \"nostatus\")\n",
    "            for t in timeline\n",
    "        ]\n",
    "\n",
    "        # Features 1: Revealed_count_diff: (unique species seen for p1) - (unique species seen for p2)\n",
    "        #     Rough proxy for how many opponent team members were revealed\n",
    "        # -----------------------------\n",
    "        p1_seen = set(p1_active)\n",
    "        p2_seen = set(p2_active)\n",
    "        feats[\"revealed_count_diff\"] = int(len(p1_seen) - len(p2_seen))\n",
    "\n",
    "        # Features 2: Used_count_diff: difference in the number of distinct species actually used\n",
    "        #     Based on Counter(p1_active) and Counter(p2_active), i.e. species that really took the field\n",
    "        # -----------------------------\n",
    "        c1 = Counter(p1_active)\n",
    "        c2 = Counter(p2_active)\n",
    "        used_count_p1 = len(c1)\n",
    "        used_count_p2 = len(c2)\n",
    "        feats[\"used_count_diff\"] = int(used_count_p1 - used_count_p2)\n",
    "\n",
    "        # -----------------------------\n",
    "        # Features 3: Hp_edge_final\n",
    "\n",
    "        last_hp_p1, last_hp_p2 = {}, {}\n",
    "        last_status_p1, last_status_p2 = {}, {}\n",
    "\n",
    "        # Track last seen HP% and status for each species of both players\n",
    "\n",
    "        for t in timeline:\n",
    "            n1 = str(t[\"p1_pokemon_state\"][\"name\"]).lower()\n",
    "            n2 = str(t[\"p2_pokemon_state\"][\"name\"]).lower()\n",
    "\n",
    "            last_hp_p1[n1] = float(t[\"p1_pokemon_state\"][\"hp_pct\"])\n",
    "            last_hp_p2[n2] = float(t[\"p2_pokemon_state\"][\"hp_pct\"])\n",
    "\n",
    "            last_status_p1[n1] = t[\"p1_pokemon_state\"].get(\"status\", \"nostatus\")\n",
    "            last_status_p2[n2] = t[\"p2_pokemon_state\"].get(\"status\", \"nostatus\")\n",
    "\n",
    "        # hp_edge_final: difference between FINAL average HP (mean over revealed species) of player 2 and player 1\n",
    "        mean_hp_p1 = float(np.mean(list(last_hp_p1.values()))) if last_hp_p1 else 0.0\n",
    "        mean_hp_p2 = float(np.mean(list(last_hp_p2.values()))) if last_hp_p2 else 0.0\n",
    "        feats[\"hp_edge_final\"] = float(mean_hp_p2 - mean_hp_p1)\n",
    "\n",
    "\n",
    "        # Features 4: P1_status_mean_final\n",
    "        # p1_status_mean_final and status_severity_gap_final\n",
    "\n",
    "        p1_status_vals = [MAP_STATUS.get(s, 0) for s in last_status_p1.values()]\n",
    "        p2_status_vals = [MAP_STATUS.get(s, 0) for s in last_status_p2.values()]\n",
    "\n",
    "        p1_status_mean_final = float(np.mean(p1_status_vals)) if p1_status_vals else 0.0\n",
    "        p2_status_mean_final = float(np.mean(p2_status_vals)) if p2_status_vals else 0.0\n",
    "\n",
    "        feats[\"p1_status_mean_final\"] = p1_status_mean_final\n",
    "\n",
    "        # Features 5: Status_severity_gap_final        \n",
    "        feats[\"status_severity_gap_final\"] = float(p2_status_mean_final - p1_status_mean_final)\n",
    "\n",
    "        rows.append(feats)\n",
    "\n",
    "    return pd.DataFrame(rows).fillna(0)\n",
    "\n",
    "# =========================\n",
    "# Our code is basically creating two dataset, a train and a test one, based on the train and test sets provided by the teachers. \n",
    "# In the train set there are 7 columns (features), that are: battle_id, player_won, \"hp_edge_final\", \"used_count_diff\", \n",
    "# \"status_severity_gap_final\", \"revealed_count_diff\" and \"p1_status_mean_final\".\n",
    "# In the test set there are 6 columns (features): the features in train set but player_won.\n",
    "# Player_won is only in the train set because is the target variable.\n",
    "# Belowe we are actually computing the features we built above for each line of our jsonl datasets (each line is a pokemon battle).\n",
    "\n",
    "\n",
    "# We are calling build_features(...) on train and test to create\n",
    "# the final tables that we will feed into the models.\n",
    "# train_df has both features and the target player_won.\n",
    "# test_df has only features (we will predict player_won for each row).\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Load raw JSONL files and create train_df / test_df with the 5 features\n",
    "# =========================\n",
    "print(f\"Loading raw JSONL:\\n- TRAIN: {TRAIN_FILE}\\n- TEST : {TEST_FILE}\")\n",
    "train_data = load_jsonl(TRAIN_FILE)\n",
    "test_data  = load_jsonl(TEST_FILE)\n",
    "print(f\"Loaded: train={len(train_data)} battles, test={len(test_data)} battles\")\n",
    "\n",
    "\n",
    "train_df = create_features1(train_data)\n",
    "test_df  = create_features1(test_data)\n",
    "print(f\"[FINAL] train_df: {train_df.shape}\")\n",
    "print(f\"[FINAL] test_df : {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb88d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10000, 5),\n",
      "y_train shape: (10000,)\n",
      "X_test  shape: (5000, 5)\n"
     ]
    }
   ],
   "source": [
    "# In this cell we have taken our variables, we have sorted them and we have insert them in X and X_test.\n",
    "# X is the numpy array computed from the train dataset (this is why it has 10k rows)\n",
    "# X_test is the numpy array computed from the test dataset (this is why it has 5k rows).\n",
    "\n",
    "# We have stored the target variable in a numpy array as well, called y.\n",
    "\n",
    "# At the end of the cell we have added a sanity check, that prints the shape of X, X_test and y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Columns that are common to both train_df and test_df\n",
    "cols = sorted(set(train_df.columns) & set(test_df.columns))\n",
    "\n",
    "# Feature columns: all common columns except identifiers/target\n",
    "f_cols = [c for c in cols if c not in (\"battle_id\", \"player_won\")]\n",
    "\n",
    "# Target vector (y) for training\n",
    "y = train_df[\"player_won\"].astype(int).to_numpy()\n",
    "\n",
    "# Feature matrices for training and test sets\n",
    "X = train_df[f_cols].to_numpy(dtype=float)\n",
    "X_test = test_df[f_cols].to_numpy(dtype=float)\n",
    "\n",
    "# Sanity check: shapes of training and test matrices\n",
    "print(f\"X_train shape: {X.shape},\\ny_train shape: {y.shape}\")\n",
    "print(f\"X_test  shape: {X_test.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ddeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TRAIN] Avvio GridSearchCV per Logistic Regression (grid esteso)...\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "best score (CV mean accuracy): 0.8426\n",
      "best hyperparameters:\n",
      "  - clf__C: 0.03\n",
      "  - clf__class_weight: None\n",
      "  - clf__n_jobs: -1\n",
      "  - clf__penalty: l2\n",
      "  - clf__solver: lbfgs\n",
      "  - clf__tol: 0.001\n",
      "\n",
      "Accuracy per fold (best hyperparameters):\n",
      "  Fold 1: 0.8365\n",
      "  Fold 2: 0.8395\n",
      "  Fold 3: 0.8535\n",
      "  Fold 4: 0.8405\n",
      "  Fold 5: 0.8430\n",
      "\n",
      "Media CV (mean_test_score): 0.8426\n",
      "Deviazione standard CV:      0.0058\n",
      "Accuracy on X: 0.8426\n",
      "\n",
      "Salvato: submission_lr2.csv\n",
      "FINE LOGISTICA MAXATA ✅\n"
     ]
    }
   ],
   "source": [
    "# In this model, we have decided to implement a simple logistic regression, with few variables for easy interpretability and\n",
    "# for keeping the complexity low. \n",
    "# We have implemented a Grid Search Cross Validation, as suggested by the professor and his TA. \n",
    "# For enanching reproducibility, we have also set a seed.\n",
    "# For further details about gridsearch, cross validation or function taken from libraries, please refer to the documentation of the respective libraries.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# We have decided first to scale our features and then to use a logistic regression\n",
    "# Pipeline: scale features -> LogisticRegression\n",
    "pipe = Pipeline([        \n",
    "    (\"scale\",  StandardScaler()),\n",
    "    (\"clf\",    LogisticRegression(\n",
    "        random_state=SEED,\n",
    "        max_iter=20000\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# In the next lines we will build some grid searches in order to find the hyperparameters that maximize the accuracy.\n",
    "# Then we will use these hyperparameters for building the final model.\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter grid for LogisticRegression:\n",
    "# 1) saga + elasticnet, 2) lbfgs + L2, 3) liblinear with L1/L2\n",
    "param_grid = [\n",
    "\n",
    "    {\n",
    "        \"clf__solver\":      [\"saga\"],\n",
    "        \"clf__penalty\":     [\"elasticnet\"],\n",
    "        \"clf__C\":           [0.003, 0.005, 0.0075, 0.009, 0.011, 0.015],\n",
    "        \"clf__l1_ratio\":    [0.25, 0.5, 0.75],\n",
    "        \"clf__tol\":         [5e-05, 1e-04, 2e-04],\n",
    "        \"clf__class_weight\":[None, \"balanced\"],\n",
    "        \"clf__n_jobs\":      [-1],\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"clf__solver\":      [\"lbfgs\"],\n",
    "        \"clf__penalty\":     [\"l2\"],\n",
    "        \"clf__C\":           [0.001, 0.003, 0.01, 0.03, 0.1, 1.0],\n",
    "        \"clf__tol\":         [1e-04, 1e-03],\n",
    "        \"clf__class_weight\":[None, \"balanced\"],\n",
    "        \"clf__n_jobs\":      [-1],\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"clf__solver\":      [\"liblinear\"],\n",
    "        \"clf__penalty\":     [\"l1\", \"l2\"],\n",
    "        \"clf__C\":           [0.001, 0.003, 0.01, 0.03, 0.1, 1.0],\n",
    "        \"clf__tol\":         [1e-04, 1e-03],\n",
    "        \"clf__class_weight\":[None, \"balanced\"],\n",
    "        \"clf__n_jobs\":      [-1],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Here we have set our cross validation, splitting the train set in five small subset\n",
    "# Stratified 5-fold CV (keeps class balance in each fold)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "\n",
    "# Grid search over the Pipeline hyperparameters using accuracy as scoring\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# Here we are runnning the grid search on the full training set\n",
    "print(\"\\n[TRAIN] Avvio GridSearchCV per Logistic Regression (grid esteso)...\")\n",
    "grid.fit(X, y)\n",
    "\n",
    "\n",
    "print(\"best score (CV mean accuracy):\", round(grid.best_score_, 4))\n",
    "\n",
    "# Inspect best hyperparameters and cross-validation scores\n",
    "print(\"best hyperparameters:\")\n",
    "for k, v in grid.best_params_.items():\n",
    "    print(f\"  - {k}: {v}\")\n",
    "\n",
    "\n",
    "\n",
    "# Convenience variables to access detailed CV results\n",
    "cv_results = grid.cv_results_\n",
    "best_idx   = grid.best_index_\n",
    "n_folds    = cv.get_n_splits()\n",
    "\n",
    "print(\"\\nAccuracy per fold (best hyperparameters):\")\n",
    "\n",
    "# Extract accuracy for each CV fold at the best hyperparameter configuration\n",
    "fold_scores = []\n",
    "for i in range(n_folds):\n",
    "    key = f\"split{i}_test_score\"\n",
    "    score_i = cv_results[key][best_idx]\n",
    "    fold_scores.append(score_i)\n",
    "    print(f\"  Fold {i+1}: {score_i:.4f}\")\n",
    "\n",
    "\n",
    "# Print mean and standard deviation of CV accuracy\n",
    "mean_score = cv_results[\"mean_test_score\"][best_idx]\n",
    "std_score  = cv_results[\"std_test_score\"][best_idx]\n",
    "print(f\"\\nMedia CV (mean_test_score): {mean_score:.4f}\")\n",
    "print(f\"Deviazione standard CV:      {std_score:.4f}\")\n",
    "\n",
    "\n",
    "# Pipeline instance with the best hyperparameters found by GridSearchCV\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "# Derive feature names: use DataFrame columns if available, otherwise generic f0, f1, ...\n",
    "if hasattr(X, \"columns\"):\n",
    "    feature_names = list(X.columns)\n",
    "else:\n",
    "    feature_names = [f\"f{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "clf = best_model.named_steps[\"clf\"]\n",
    "coefs = clf.coef_[0]\n",
    "\n",
    "\n",
    "# Build a DataFrame with LR coefficients and sort them by absolute value\n",
    "coef_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"coef\": coefs,\n",
    "        \"abs_coef\": np.abs(coefs)\n",
    "    })\n",
    "    .sort_values(\"abs_coef\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Here we are computing predictions on the training-set. We have computed them in order to make the reader have an idea on how good our model, when \n",
    "# applied to the train, is good at predicting the result (we are basically computing accuracy on X).\n",
    "# This accuracy is being reported just for the seek of completness, but the most relevant and important accuracy is the one on the Cross validation, that\n",
    "# has been printed with the code above.\n",
    "y_train_pred = best_model.predict(X)\n",
    "\n",
    "train_accuracy = accuracy_score(y, y_train_pred)\n",
    "print(f\"Accuracy on X: {train_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Refit the best model on the full training data and generate predictions for the test set\n",
    "best_model.fit(X, y)\n",
    "test_pred = best_model.predict(X_test).astype(int)\n",
    "\n",
    "\n",
    "# Here we are building the submission DataFrame for the competition/evaluation.\n",
    "submission = pd.DataFrame({\n",
    "    \"battle_id\": test_df[\"battle_id\"],\n",
    "    \"player_won\": test_pred\n",
    "})\n",
    "submission.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "print(\"\\nSalvato: submission_lr2.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
